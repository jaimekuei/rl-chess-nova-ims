{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 07:41:45.325541: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import chess\n",
    "import gym\n",
    "import gym_chess\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import save_checkpoint, update_game_metrics, load_checkpoint, get_custom_reward\n",
    "from agents import QLearningAgent\n",
    "\n",
    "from stockfish import Stockfish\n",
    "import argparse\n",
    "import yaml\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jaime.kuei/Documents/study-repositories/master/second-semester/reinforcement-learning/rl-chess-nova-ims/checkpoint/dqn_white_stockfish/v1/iteration_800/checkpoint_agent.pkl\n",
      "Loading checkpoint from:  iteration_800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> Loaded Q_dict_white\n",
      "----> Loaded game_metrics\n"
     ]
    }
   ],
   "source": [
    "strategy = 'dqn_white_stockfish'\n",
    "version = 'v1'\n",
    "\n",
    "checkpoint = load_checkpoint(strategy, version, iteration='latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = checkpoint['agent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 495,\n",
       " 496,\n",
       " 497,\n",
       " 498,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 530,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 539,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 545,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 559,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 571,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 576,\n",
       " 577,\n",
       " 578,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 587,\n",
       " 588,\n",
       " 589,\n",
       " 590,\n",
       " 591,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 596,\n",
       " 597,\n",
       " 598,\n",
       " 599,\n",
       " 600,\n",
       " 601,\n",
       " 602,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 606,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 610,\n",
       " 611,\n",
       " 612,\n",
       " 613,\n",
       " 614,\n",
       " 615,\n",
       " 616,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 620,\n",
       " 621,\n",
       " 622,\n",
       " 623,\n",
       " 624,\n",
       " 625,\n",
       " 626,\n",
       " 627,\n",
       " 628,\n",
       " 629,\n",
       " 630,\n",
       " 631,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 635,\n",
       " 636,\n",
       " 637,\n",
       " 638,\n",
       " 639,\n",
       " 640,\n",
       " 641,\n",
       " 642,\n",
       " 643,\n",
       " 644,\n",
       " 645,\n",
       " 646,\n",
       " 647,\n",
       " 648,\n",
       " 649,\n",
       " 650,\n",
       " 651,\n",
       " 652,\n",
       " 653,\n",
       " 654,\n",
       " 655,\n",
       " 656,\n",
       " 657,\n",
       " 658,\n",
       " 659,\n",
       " 660,\n",
       " 661,\n",
       " 662,\n",
       " 663,\n",
       " 664,\n",
       " 665,\n",
       " 666,\n",
       " 667,\n",
       " 668,\n",
       " 669,\n",
       " 670,\n",
       " 671,\n",
       " 672,\n",
       " 673,\n",
       " 674,\n",
       " 675,\n",
       " 676,\n",
       " 677,\n",
       " 678,\n",
       " 679,\n",
       " 680,\n",
       " 681,\n",
       " 682,\n",
       " 683,\n",
       " 684,\n",
       " 685,\n",
       " 686,\n",
       " 687,\n",
       " 688,\n",
       " 689,\n",
       " 690,\n",
       " 691,\n",
       " 692,\n",
       " 693,\n",
       " 694,\n",
       " 695,\n",
       " 696,\n",
       " 697,\n",
       " 698,\n",
       " 699,\n",
       " 700,\n",
       " 701,\n",
       " 702,\n",
       " 703,\n",
       " 704,\n",
       " 705,\n",
       " 706,\n",
       " 707,\n",
       " 708,\n",
       " 709,\n",
       " 710,\n",
       " 711,\n",
       " 712,\n",
       " 713,\n",
       " 714,\n",
       " 715,\n",
       " 716,\n",
       " 717,\n",
       " 718,\n",
       " 719,\n",
       " 720,\n",
       " 721,\n",
       " 722,\n",
       " 723,\n",
       " 724,\n",
       " 725,\n",
       " 726,\n",
       " 727,\n",
       " 728,\n",
       " 729,\n",
       " 730,\n",
       " 731,\n",
       " 732,\n",
       " 733,\n",
       " 734,\n",
       " 735,\n",
       " 736,\n",
       " 737,\n",
       " 738,\n",
       " 739,\n",
       " 740,\n",
       " 741,\n",
       " 742,\n",
       " 743,\n",
       " 744,\n",
       " 745,\n",
       " 746,\n",
       " 747,\n",
       " 748,\n",
       " 749,\n",
       " 750,\n",
       " 751,\n",
       " 752,\n",
       " 753,\n",
       " 754,\n",
       " 755,\n",
       " 756,\n",
       " 757,\n",
       " 758,\n",
       " 759,\n",
       " 760,\n",
       " 761,\n",
       " 762,\n",
       " 763,\n",
       " 764,\n",
       " 765,\n",
       " 766,\n",
       " 767,\n",
       " 768,\n",
       " 769,\n",
       " 770,\n",
       " 771,\n",
       " 772,\n",
       " 773,\n",
       " 774,\n",
       " 775,\n",
       " 776,\n",
       " 777,\n",
       " 778,\n",
       " 779,\n",
       " 780,\n",
       " 781,\n",
       " 782,\n",
       " 783,\n",
       " 784,\n",
       " 785,\n",
       " 786,\n",
       " 787,\n",
       " 788,\n",
       " 789,\n",
       " 790,\n",
       " 791,\n",
       " 792,\n",
       " 793,\n",
       " 794,\n",
       " 795,\n",
       " 796,\n",
       " 797,\n",
       " 798,\n",
       " 799,\n",
       " 800]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['game_metrics']['game']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 1, 1, 0],\n",
       "        [0, 1, 0, ..., 1, 1, 0],\n",
       "        [0, 0, 1, ..., 1, 1, 0],\n",
       "        ...,\n",
       "        [0, 0, 1, ..., 1, 1, 0],\n",
       "        [0, 1, 0, ..., 1, 1, 0],\n",
       "        [0, 0, 0, ..., 1, 1, 0]],\n",
       "\n",
       "       [[1, 0, 0, ..., 1, 1, 0],\n",
       "        [1, 0, 0, ..., 1, 1, 0],\n",
       "        [1, 0, 0, ..., 1, 1, 0],\n",
       "        ...,\n",
       "        [1, 0, 0, ..., 1, 1, 0],\n",
       "        [1, 0, 0, ..., 1, 1, 0],\n",
       "        [1, 0, 0, ..., 1, 1, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 1, 1, 0],\n",
       "        [0, 0, 0, ..., 1, 1, 0],\n",
       "        [0, 0, 0, ..., 1, 1, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 1, 1, 0],\n",
       "        [0, 0, 0, ..., 1, 1, 0],\n",
       "        [0, 0, 0, ..., 1, 1, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 1, 1, 0],\n",
       "        [0, 0, 0, ..., 1, 1, 0],\n",
       "        [0, 0, 0, ..., 1, 1, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 1, 1, 0],\n",
       "        [0, 0, 0, ..., 1, 1, 0],\n",
       "        [0, 0, 0, ..., 1, 1, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 1, 1, 0],\n",
       "        [0, 0, 0, ..., 1, 1, 0],\n",
       "        [0, 0, 0, ..., 1, 1, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 1, 1, 0],\n",
       "        [0, 0, 0, ..., 1, 1, 0],\n",
       "        [0, 0, 0, ..., 1, 1, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 1, 1, 0],\n",
       "        [0, 0, 0, ..., 1, 1, 0],\n",
       "        [0, 0, 0, ..., 1, 1, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 1, 1, 0],\n",
       "        [0, 0, 0, ..., 1, 1, 0],\n",
       "        [0, 0, 0, ..., 1, 1, 0]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('ChessAlphaZero-v0')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Box' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env\u001b[39m.\u001b[39;49mobservation_space[:,:,:\u001b[39m12\u001b[39;49m]\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Box' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state, reward, done, _ = env.step(730)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "state1 = next_state[:,:,:12]\n",
    "state2 = next_state[:,:,:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[494,\n",
       " 501,\n",
       " 129,\n",
       " 136,\n",
       " 1095,\n",
       " 1022,\n",
       " 949,\n",
       " 876,\n",
       " 803,\n",
       " 730,\n",
       " 657,\n",
       " 584,\n",
       " 1096,\n",
       " 1023,\n",
       " 950,\n",
       " 877,\n",
       " 804,\n",
       " 731,\n",
       " 658,\n",
       " 585]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.legal_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_buffer = deque(maxlen=1000)\n",
    "\n",
    "state, reward, done, _ = env.step(730)\n",
    "action = 731\n",
    "next_state, reward, done, _ = env.step(action)\n",
    "replay_buffer.append((state, action, reward, done, next_state))\n",
    "state = next_state\n",
    "\n",
    "action = 804\n",
    "next_state, reward, done, _ = env.step(action)\n",
    "replay_buffer.append((state, action, reward, done, next_state))\n",
    "state = next_state\n",
    "\n",
    "action = 950\n",
    "next_state, reward, done, _ = env.step(action)\n",
    "replay_buffer.append((state, action, reward, done, next_state))\n",
    "state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch = np.array(random.sample(replay_buffer, 3), dtype=object)\n",
    "state_list = np.array(minibatch[:,0], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_list = np.hstack(state_list).reshape(3, 8, 8, 119)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 8, 8, 119)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_transistion(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.store_transistion(state, action, reward, next_state, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4672)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'piece_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env\u001b[39m.\u001b[39;49mdecode(\u001b[39m2000\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/study-repositories/master/second-semester/reinforcement-learning/rl-chess-nova-ims/.reinforcement-learning-env/lib/python3.9/site-packages/gym_chess/alphazero/move_encoding/__init__.py:123\u001b[0m, in \u001b[0;36mMoveEncoding.decode\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    116\u001b[0m is_promoting_move \u001b[39m=\u001b[39m (\n\u001b[1;32m    117\u001b[0m     (to_rank \u001b[39m==\u001b[39m \u001b[39m7\u001b[39m \u001b[39mand\u001b[39;00m turn \u001b[39m==\u001b[39m chess\u001b[39m.\u001b[39mWHITE) \u001b[39mor\u001b[39;00m \n\u001b[1;32m    118\u001b[0m     (to_rank \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m turn \u001b[39m==\u001b[39m chess\u001b[39m.\u001b[39mBLACK)\n\u001b[1;32m    119\u001b[0m )\n\u001b[1;32m    122\u001b[0m piece \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munwrapped\u001b[39m.\u001b[39m_board\u001b[39m.\u001b[39mpiece_at(move\u001b[39m.\u001b[39mfrom_square)\n\u001b[0;32m--> 123\u001b[0m is_pawn \u001b[39m=\u001b[39m piece\u001b[39m.\u001b[39;49mpiece_type \u001b[39m==\u001b[39m chess\u001b[39m.\u001b[39mPAWN\n\u001b[1;32m    125\u001b[0m \u001b[39mif\u001b[39;00m is_pawn \u001b[39mand\u001b[39;00m is_promoting_move:\n\u001b[1;32m    126\u001b[0m     move\u001b[39m.\u001b[39mpromotion \u001b[39m=\u001b[39m chess\u001b[39m.\u001b[39mQUEEN\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'piece_type'"
     ]
    }
   ],
   "source": [
    "env.decode(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3001\n",
      "2872\n",
      "3871\n",
      "2648\n",
      "3275\n",
      "571\n",
      "2459\n",
      "2741\n",
      "4047\n",
      "2999\n",
      "485\n",
      "1269\n",
      "3593\n",
      "4219\n",
      "524\n",
      "2313\n",
      "2480\n",
      "2834\n",
      "3856\n",
      "132\n",
      "4035\n",
      "2262\n",
      "1349\n",
      "2649\n",
      "3454\n",
      "2178\n",
      "72\n",
      "790\n",
      "1201\n",
      "479\n",
      "623\n",
      "2407\n",
      "3599\n",
      "1970\n",
      "4322\n",
      "1699\n",
      "1757\n",
      "4179\n",
      "3450\n",
      "918\n",
      "4368\n",
      "3722\n",
      "3849\n",
      "3545\n",
      "924\n",
      "247\n",
      "3436\n",
      "4192\n",
      "1627\n",
      "1553\n",
      "870\n",
      "2498\n",
      "4374\n",
      "4559\n",
      "683\n",
      "2336\n",
      "4601\n",
      "474\n",
      "3906\n",
      "4103\n",
      "3731\n",
      "2622\n",
      "1178\n",
      "2416\n",
      "3290\n",
      "4134\n",
      "2342\n",
      "4358\n",
      "4041\n",
      "2162\n",
      "494\n",
      "2037\n",
      "2351\n",
      "270\n",
      "1222\n",
      "3390\n",
      "2788\n",
      "2389\n",
      "3644\n",
      "2396\n",
      "1623\n",
      "635\n",
      "4188\n",
      "238\n",
      "3592\n",
      "359\n",
      "4579\n",
      "3430\n",
      "1857\n",
      "2008\n",
      "3848\n",
      "2053\n",
      "645\n",
      "2758\n",
      "4206\n",
      "2489\n",
      "2332\n",
      "4042\n",
      "3015\n",
      "1100\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4385: 0.9997244926071289,\n",
       " 2217: 0.999632656809581,\n",
       " 49: 0.9995408210120331,\n",
       " 4434: 0.999265313619162,\n",
       " 2266: 0.9991734778218415,\n",
       " 98: 0.9990816420240662,\n",
       " 4483: 0.9988061346311952,\n",
       " 2315: 0.9987142988338746,\n",
       " 147: 0.9986224630360994,\n",
       " 4532: 0.998346955643683,\n",
       " 2364: 0.9982551198459078,\n",
       " 196: 0.9981632840481325,\n",
       " 4581: 0.9978877766552614,\n",
       " 2413: 0.9977959408579409,\n",
       " 245: 0.9977041050601656,\n",
       " 4630: 0.9974285976677493,\n",
       " 2462: 0.997336761869974,\n",
       " 294: 0.9972449260721987,\n",
       " 4679: 0.9969694186793276,\n",
       " 2511: 0.9968775828820071,\n",
       " 343: 0.9967857470842318,\n",
       " 4728: 0.9965102396918155,\n",
       " 2560: 0.9964184038940402,\n",
       " 392: 0.996326568096265,\n",
       " 4777: 0.9960510607033939,\n",
       " 2609: 0.9959592249060734,\n",
       " 441: 0.9958673891082981,\n",
       " 4826: 0.9955918817158818,\n",
       " 2658: 0.9955000459181065,\n",
       " 490: 0.9954082101203312,\n",
       " 4875: 0.9951327027274601,\n",
       " 2707: 0.9950408669301396,\n",
       " 539: 0.9949490311323643,\n",
       " 4924: 0.994673523739948,\n",
       " 2756: 0.9945816879421727,\n",
       " 588: 0.9944898521443974,\n",
       " 4973: 0.9942143447515264,\n",
       " 2805: 0.9941225089542058,\n",
       " 637: 0.9940306731564306,\n",
       " 2854: 0.993663329966239,\n",
       " 686: 0.9935714941684637,\n",
       " 2903: 0.9932041509782721,\n",
       " 735: 0.9931123151804968,\n",
       " 2952: 0.9927449719903052,\n",
       " 784: 0.9926531361925299,\n",
       " 3001: 0.9922857930023383,\n",
       " 833: 0.992193957204563,\n",
       " 3050: 0.9918266140143714,\n",
       " 882: 0.9917347782165962,\n",
       " 3099: 0.9913674350264046,\n",
       " 931: 0.9912755992286293,\n",
       " 3148: 0.9909082560384377,\n",
       " 980: 0.9908164202406624,\n",
       " 3197: 0.9904490770504708,\n",
       " 1029: 0.9903572412526955,\n",
       " 3246: 0.9899898980625039,\n",
       " 1078: 0.9898980622647287,\n",
       " 3295: 0.989530719074537,\n",
       " 1127: 0.9894388832767618,\n",
       " 3344: 0.9890715400865702,\n",
       " 1176: 0.9889797042887949,\n",
       " 3393: 0.9886123610986033,\n",
       " 1225: 0.988520525300828,\n",
       " 3442: 0.9881531821106364,\n",
       " 1274: 0.9880613463128611,\n",
       " 3491: 0.9876940031226695,\n",
       " 1323: 0.9876021673248943,\n",
       " 3540: 0.9872348241347026,\n",
       " 1372: 0.9871429883369274,\n",
       " 3589: 0.9867756451467358,\n",
       " 1421: 0.9866838093489605,\n",
       " 3638: 0.9863164661587689,\n",
       " 1470: 0.9862246303609936,\n",
       " 3687: 0.985857287170802,\n",
       " 1519: 0.9857654513730267,\n",
       " 3736: 0.9853981081828351,\n",
       " 1568: 0.9853062723850599,\n",
       " 3785: 0.9849389291948683,\n",
       " 1617: 0.984847093397093,\n",
       " 3834: 0.9844797502069014,\n",
       " 1666: 0.9843879144091261,\n",
       " 3883: 0.9840205712189345,\n",
       " 1715: 0.9839287354211592,\n",
       " 3932: 0.9835613922309676,\n",
       " 1764: 0.9834695564331923,\n",
       " 3981: 0.9831022132430007,\n",
       " 1813: 0.9830103774452255,\n",
       " 4030: 0.9826430342550339,\n",
       " 1862: 0.9825511984572586,\n",
       " 4079: 0.982183855267067,\n",
       " 1911: 0.9820920194692917,\n",
       " 4128: 0.9817246762791001,\n",
       " 1960: 0.9816328404813248,\n",
       " 4177: 0.9812654972911332,\n",
       " 2009: 0.9811736614933579,\n",
       " 4226: 0.9808063183031663,\n",
       " 2058: 0.9807144825053911,\n",
       " 4275: 0.9803471393151995,\n",
       " 2107: 0.9802553035174242,\n",
       " 4324: 0.9798879603272326,\n",
       " 2156: 0.9797961245294573,\n",
       " 4373: 0.9794287813392657,\n",
       " 2205: 0.9793369455414904,\n",
       " 37: 0.9792451097437791,\n",
       " 4422: 0.9789696023512988,\n",
       " 2254: 0.9788777665535235,\n",
       " 86: 0.9787859307558051,\n",
       " 4471: 0.9785104233633319,\n",
       " 2303: 0.9784185875655567,\n",
       " 135: 0.9783267517678382,\n",
       " 4520: 0.9780512443749103,\n",
       " 2352: 0.9779594085775898,\n",
       " 184: 0.9778675727798714,\n",
       " 4569: 0.9775920653873982,\n",
       " 2401: 0.9775002295896229,\n",
       " 233: 0.9774083937919045,\n",
       " 4618: 0.9771328863989766,\n",
       " 2450: 0.977041050601656,\n",
       " 282: 0.9769492148039376,\n",
       " 4667: 0.9766737074114644,\n",
       " 2499: 0.9765818716136891,\n",
       " 331: 0.9764900358159707,\n",
       " 4716: 0.9762145284230428,\n",
       " 2548: 0.9761226926257223,\n",
       " 380: 0.9760308568280038,\n",
       " 4765: 0.9757553494355307,\n",
       " 2597: 0.9756635136377554,\n",
       " 429: 0.975571677840037,\n",
       " 4814: 0.975296170447109,\n",
       " 2646: 0.9752043346497885,\n",
       " 478: 0.9751124988520701,\n",
       " 4863: 0.9748369914595969,\n",
       " 2695: 0.9747451556618216,\n",
       " 527: 0.9746533198641032,\n",
       " 4912: 0.9743778124711753,\n",
       " 2744: 0.9742859766738547,\n",
       " 576: 0.9741941408760795,\n",
       " 4961: 0.9739186334836631,\n",
       " 2793: 0.9738267976858879,\n",
       " 625: 0.9737349618881126,\n",
       " 2842: 0.973367618697921,\n",
       " 674: 0.9732757829001457,\n",
       " 2891: 0.9729084397099541,\n",
       " 723: 0.9728166039121788,\n",
       " 2940: 0.9724492607219872,\n",
       " 772: 0.972357424924212,\n",
       " 2989: 0.9719900817340204,\n",
       " 821: 0.9718982459362451,\n",
       " 3038: 0.9715309027460535,\n",
       " 870: 0.9714390669482782,\n",
       " 3087: 0.9710717237580866,\n",
       " 919: 0.9709798879603113,\n",
       " 3136: 0.9706125447701197,\n",
       " 968: 0.9705207089723444,\n",
       " 3185: 0.9701533657821528,\n",
       " 1017: 0.9700615299843776,\n",
       " 3234: 0.969694186794186,\n",
       " 1066: 0.9696023509964107,\n",
       " 3283: 0.9692350078062191,\n",
       " 1115: 0.9691431720084438,\n",
       " 3332: 0.9687758288182522,\n",
       " 1164: 0.9686839930204769,\n",
       " 3381: 0.9683166498302853,\n",
       " 1213: 0.96822481403251,\n",
       " 3430: 0.9678574708423184,\n",
       " 1262: 0.9677656350445432,\n",
       " 3479: 0.9673982918543516,\n",
       " 1311: 0.9673064560565763,\n",
       " 3528: 0.9669391128663847,\n",
       " 1360: 0.9668472770686094,\n",
       " 3577: 0.9664799338784178,\n",
       " 1409: 0.9663880980806425,\n",
       " 3626: 0.9660207548904509,\n",
       " 1458: 0.9659289190926756,\n",
       " 3675: 0.965561575902484,\n",
       " 1507: 0.9654697401047088,\n",
       " 3724: 0.9651023969145172,\n",
       " 1556: 0.9650105611167419,\n",
       " 3773: 0.9646432179265503,\n",
       " 1605: 0.964551382128775,\n",
       " 3822: 0.9641840389385834,\n",
       " 1654: 0.9640922031408081,\n",
       " 3871: 0.9637248599506165,\n",
       " 1703: 0.9636330241528412,\n",
       " 3920: 0.9632656809626496,\n",
       " 1752: 0.9631738451648744,\n",
       " 3969: 0.9628065019746828,\n",
       " 1801: 0.9627146661769075,\n",
       " 4018: 0.9623473229867159,\n",
       " 1850: 0.9622554871889406,\n",
       " 4067: 0.961888143998749,\n",
       " 1899: 0.9617963082009737,\n",
       " 4116: 0.9614289650107821,\n",
       " 1948: 0.9613371292130068,\n",
       " 4165: 0.9609697860228152,\n",
       " 1997: 0.96087795022504,\n",
       " 4214: 0.9605106070348484,\n",
       " 2046: 0.9604187712370731,\n",
       " 4263: 0.9600514280468815,\n",
       " 2095: 0.9599595922491062,\n",
       " 4312: 0.9595922490589146,\n",
       " 2144: 0.9595004132611393,\n",
       " 4361: 0.9591330700709477,\n",
       " 2193: 0.9590412342731724,\n",
       " 25: 0.9589493984755251,\n",
       " 4410: 0.9586738910829808,\n",
       " 2242: 0.9585820552852056,\n",
       " 74: 0.9584902194875582,\n",
       " 4459: 0.958214712095014,\n",
       " 2291: 0.9581228762972387,\n",
       " 123: 0.9580310404995913,\n",
       " 4508: 0.9577555331070471,\n",
       " 2340: 0.9576636973092718,\n",
       " 172: 0.9575718615116102,\n",
       " 4557: 0.9572963541186255,\n",
       " 2389: 0.9572045183213049,\n",
       " 221: 0.9571126825236433,\n",
       " 4606: 0.9568371751311133,\n",
       " 2438: 0.956745339333338,\n",
       " 270: 0.9566535035356765,\n",
       " 4655: 0.9563779961426917,\n",
       " 2487: 0.9562861603453712,\n",
       " 319: 0.9561943245477096,\n",
       " 4704: 0.9559188171551796,\n",
       " 2536: 0.9558269813574043,\n",
       " 368: 0.9557351455597427,\n",
       " 4753: 0.9554596381667579,\n",
       " 2585: 0.9553678023694374,\n",
       " 417: 0.9552759665717758,\n",
       " 4802: 0.9550004591792458,\n",
       " 2634: 0.9549086233814705,\n",
       " 466: 0.954816787583809,\n",
       " 4851: 0.9545412801908242,\n",
       " 2683: 0.9544494443935037,\n",
       " 515: 0.9543576085958421,\n",
       " 4900: 0.954082101203312,\n",
       " 2732: 0.9539902654055368,\n",
       " 564: 0.9538984296078752,\n",
       " 4949: 0.9536229222148904,\n",
       " 2781: 0.9535310864175699,\n",
       " 613: 0.9534392506199083,\n",
       " 4998: 0.9531637432273783,\n",
       " 2830: 0.953071907429603,\n",
       " 662: 0.9529800716319414,\n",
       " 2879: 0.9526127284416361,\n",
       " 711: 0.9525208926439745,\n",
       " 2928: 0.9521535494536693,\n",
       " 760: 0.9520617136560077,\n",
       " 2977: 0.9516943704657024,\n",
       " 809: 0.9516025346680408,\n",
       " 3026: 0.9512351914777355,\n",
       " 858: 0.9511433556800739,\n",
       " 3075: 0.9507760124897686,\n",
       " 907: 0.950684176692107,\n",
       " 3124: 0.9503168335018017,\n",
       " 956: 0.9502249977041402,\n",
       " 3173: 0.9498576545138349,\n",
       " 1005: 0.9497658187161733,\n",
       " 3222: 0.949398475525868,\n",
       " 1054: 0.9493066397282064,\n",
       " 3271: 0.9489392965379011,\n",
       " 1103: 0.9488474607402395,\n",
       " 3320: 0.9484801175499342,\n",
       " 1152: 0.948388281752159,\n",
       " 3369: 0.9480209385619673,\n",
       " 1201: 0.9479291027641921,\n",
       " 3418: 0.9475617595740005,\n",
       " 1250: 0.9474699237762252,\n",
       " 3467: 0.9471025805860336,\n",
       " 1299: 0.9470107447882583,\n",
       " 3516: 0.9466434015980667,\n",
       " 1348: 0.9465515658002914,\n",
       " 3565: 0.9461842226100998,\n",
       " 1397: 0.9460923868123245,\n",
       " 3614: 0.9457250436221329,\n",
       " 1446: 0.9456332078243577,\n",
       " 3663: 0.9452658646341661,\n",
       " 1495: 0.9451740288363908,\n",
       " 3712: 0.9448066856461992,\n",
       " 1544: 0.9447148498484239,\n",
       " 3761: 0.9443475066582323,\n",
       " 1593: 0.944255670860457,\n",
       " 3810: 0.9438883276702654,\n",
       " 1642: 0.9437964918724902,\n",
       " 3859: 0.9434291486822985,\n",
       " 1691: 0.9433373128845233,\n",
       " 3908: 0.9429699696943317,\n",
       " 1740: 0.9428781338965564,\n",
       " 3957: 0.9425107907063648,\n",
       " 1789: 0.9424189549085895,\n",
       " 4006: 0.9420516117183979,\n",
       " 1838: 0.9419597759206226,\n",
       " 4055: 0.941592432730431,\n",
       " 1887: 0.9415005969326558,\n",
       " 4104: 0.9411332537424641,\n",
       " 1936: 0.9410414179446889,\n",
       " 4153: 0.9406740747544973,\n",
       " 1985: 0.940582238956722,\n",
       " 4202: 0.9402148957665304,\n",
       " 2034: 0.9401230599687551,\n",
       " 4251: 0.9397557167785635,\n",
       " 2083: 0.9396638809807882,\n",
       " 4300: 0.9392965377905966,\n",
       " 2132: 0.9392047019928214,\n",
       " 4349: 0.9388373588026298,\n",
       " 2181: 0.9387455230048545,\n",
       " 13: 0.9386536872072728,\n",
       " 4398: 0.9383781798146629,\n",
       " 2230: 0.9382863440168876,\n",
       " 62: 0.9381945082193042,\n",
       " 4447: 0.937919000826696,\n",
       " 2279: 0.9378271650289207,\n",
       " 111: 0.9377353292313302,\n",
       " 4496: 0.9374598218382744,\n",
       " 2328: 0.9373679860409538,\n",
       " 160: 0.9372761502433775,\n",
       " 4545: 0.9370006428507622,\n",
       " 2377: 0.936908807052987,\n",
       " 209: 0.9368169712554106,\n",
       " 4594: 0.9365414638623406,\n",
       " 2426: 0.9364496280650201,\n",
       " 258: 0.9363577922674438,\n",
       " 4643: 0.9360822848748285,\n",
       " 2475: 0.9359904490770532,\n",
       " 307: 0.9358986132794485,\n",
       " 4692: 0.9356231058864068,\n",
       " 2524: 0.9355312700890863,\n",
       " 356: 0.9354394342914816,\n",
       " 4741: 0.9351639268988947,\n",
       " 2573: 0.9350720911011194,\n",
       " 405: 0.9349802553035147,\n",
       " 4790: 0.9347047479104731,\n",
       " 2622: 0.9346129121131526,\n",
       " 454: 0.9345210763155478,\n",
       " 4839: 0.934245568922961,\n",
       " 2671: 0.9341537331251857,\n",
       " 503: 0.9340618973275809,\n",
       " 4888: 0.9337863899345393,\n",
       " 2720: 0.9336945541372188,\n",
       " 552: 0.9336027183396141,\n",
       " 4937: 0.9333272109470272,\n",
       " 2769: 0.9332353751492519,\n",
       " 601: 0.933143539351704,\n",
       " 4986: 0.9328680319586056,\n",
       " 2818: 0.932776196161285,\n",
       " 650: 0.9326843603637371,\n",
       " 2867: 0.9323170171733182,\n",
       " 699: 0.9322251813756566,\n",
       " 2916: 0.9318578381853513,\n",
       " 748: 0.9317660023876897,\n",
       " 2965: 0.9313986591973844,\n",
       " 797: 0.9313068233997228,\n",
       " 3014: 0.9309394802094175,\n",
       " 846: 0.9308476444117559,\n",
       " 3063: 0.9304803012214506,\n",
       " 895: 0.9303884654237891,\n",
       " 3112: 0.9300211222334838,\n",
       " 944: 0.9299292864358222,\n",
       " 3161: 0.9295619432455169,\n",
       " 993: 0.9294701074478553,\n",
       " 3210: 0.92910276425755,\n",
       " 1042: 0.9290109284598884,\n",
       " 3259: 0.9286435852695831,\n",
       " 1091: 0.9285517494719215,\n",
       " 3308: 0.9281844062816162,\n",
       " 1140: 0.9280925704840683,\n",
       " 3357: 0.9277252272936494,\n",
       " 1189: 0.9276333914961015,\n",
       " 3406: 0.9272660483056825,\n",
       " 1238: 0.9271742125081346,\n",
       " 3455: 0.9268068693177156,\n",
       " 1287: 0.9267150335201677,\n",
       " 3504: 0.9263476903297487,\n",
       " 1336: 0.9262558545322008,\n",
       " 3553: 0.9258885113417819,\n",
       " 1385: 0.925796675544234,\n",
       " 3602: 0.925429332353815,\n",
       " 1434: 0.9253374965562671,\n",
       " 3651: 0.9249701533658481,\n",
       " 1483: 0.9248783175683002,\n",
       " 3700: 0.9245109743778812,\n",
       " 1532: 0.9244191385803333,\n",
       " 3749: 0.9240517953899143,\n",
       " 1581: 0.9239599595923664,\n",
       " 3798: 0.9235926164019475,\n",
       " 1630: 0.9235007806043996,\n",
       " 3847: 0.9231334374139806,\n",
       " 1679: 0.9230416016164327,\n",
       " 3896: 0.9226742584260137,\n",
       " 1728: 0.9225824226284658,\n",
       " 3945: 0.9222150794380468,\n",
       " 1777: 0.9221232436404989,\n",
       " 3994: 0.9217559004500799,\n",
       " 1826: 0.921664064652532,\n",
       " 4043: 0.921296721462113,\n",
       " 1875: 0.9212048856645652,\n",
       " 4092: 0.9208375424741462,\n",
       " 1924: 0.9207457066765983,\n",
       " 4141: 0.9203783634861793,\n",
       " 1973: 0.9202865276886314,\n",
       " 4190: 0.9199191844982124,\n",
       " 2022: 0.9198273487006645,\n",
       " 4239: 0.9194600055102455,\n",
       " 2071: 0.9193681697126976,\n",
       " 4288: 0.9190008265222787,\n",
       " 2120: 0.9189089907247308,\n",
       " 4337: 0.9185416475343118,\n",
       " 2169: 0.9184498117367639,\n",
       " 1: 0.918357975939021,\n",
       " 4386: 0.9180824685463449,\n",
       " 2218: 0.917990632748797,\n",
       " 50: 0.9178987969510501,\n",
       " 4435: 0.917623289558378,\n",
       " 2267: 0.9175314537606027,\n",
       " 99: 0.9174396179630833,\n",
       " 4484: 0.9171641105704111,\n",
       " 2316: 0.9170722747726359,\n",
       " 148: 0.9169804389751164,\n",
       " 4533: 0.9167049315819895,\n",
       " 2365: 0.916613095784669,\n",
       " 197: 0.9165212599871495,\n",
       " 4582: 0.9162457525944774,\n",
       " 2414: 0.9161539167967021,\n",
       " 246: 0.9160620809991826,\n",
       " 4631: 0.9157865736060558,\n",
       " 2463: 0.9156947378087352,\n",
       " 295: 0.9156029020111873,\n",
       " 4680: 0.9153273946185436,\n",
       " 2512: 0.9152355588207683,\n",
       " 344: 0.9151437230232204,\n",
       " 4729: 0.914868215630122,\n",
       " 2561: 0.9147763798328015,\n",
       " 393: 0.9146845440352536,\n",
       " 4778: 0.9144090366426099,\n",
       " 2610: 0.9143172008448346,\n",
       " 442: 0.9142253650472867,\n",
       " 4827: 0.9139498576541882,\n",
       " 2659: 0.9138580218568677,\n",
       " 491: 0.9137661860593198,\n",
       " 4876: 0.9134906786666761,\n",
       " 2708: 0.9133988428689008,\n",
       " 540: 0.9133070070713529,\n",
       " 4925: 0.9130314996782545,\n",
       " 2757: 0.912939663880934,\n",
       " 589: 0.912847828083386,\n",
       " 4974: 0.9125723206907423,\n",
       " 2806: 0.9124804848929671,\n",
       " 638: 0.9123886490954192,\n",
       " 2855: 0.9120213059050002,\n",
       " 687: 0.9119294701074523,\n",
       " 2904: 0.9115621269170333,\n",
       " 736: 0.9114702911194854,\n",
       " 2953: 0.9111029479290664,\n",
       " 785: 0.9110111121315185,\n",
       " 3002: 0.9106437689410996,\n",
       " 834: 0.9105519331435517,\n",
       " 3051: 0.9101845899531327,\n",
       " 883: 0.9100927541555848,\n",
       " 3100: 0.9097254109651658,\n",
       " 932: 0.9096335751676179,\n",
       " 3149: 0.9092662319771989,\n",
       " 981: 0.909174396179651,\n",
       " 3198: 0.908807052989232,\n",
       " 1030: 0.9087152171916841,\n",
       " 3247: 0.9083478740012652,\n",
       " 1079: 0.9082560382037173,\n",
       " 3296: 0.9078886950132983,\n",
       " 1128: 0.9077968592157504,\n",
       " 3345: 0.9074295160253314,\n",
       " 1177: 0.9073376802277835,\n",
       " 3394: 0.9069703370373645,\n",
       " 1226: 0.9068785012398166,\n",
       " 3443: 0.9065111580493976,\n",
       " 1275: 0.9064193222518497,\n",
       " 3492: 0.9060519790614308,\n",
       " 1324: 0.9059601432638829,\n",
       " 3541: 0.9055928000734639,\n",
       " 1373: 0.905500964275916,\n",
       " 3590: 0.905133621085497,\n",
       " 1422: 0.9050417852879491,\n",
       " 3639: 0.9046744420975301,\n",
       " 1471: 0.9045826062999822,\n",
       " 3688: 0.9042152631095632,\n",
       " 1520: 0.9041234273120153,\n",
       " 3737: 0.9037560841215964,\n",
       " 1569: 0.9036642483240485,\n",
       " 3786: 0.9032969051336295,\n",
       " 1618: 0.9032050693360816,\n",
       " 3835: 0.9028377261456626,\n",
       " 1667: 0.9027458903481147,\n",
       " 3884: 0.9023785471576957,\n",
       " 1716: 0.9022867113601478,\n",
       " 3933: 0.9019193681697288,\n",
       " 1765: 0.9018275323721809,\n",
       " 3982: 0.901460189181762,\n",
       " 1814: 0.9013683533842141,\n",
       " 4031: 0.9010010101937951,\n",
       " 1863: 0.9009091743962472,\n",
       " 4080: 0.9005418312058282,\n",
       " 1912: 0.9004499954082803,\n",
       " 4129: 0.9000826522178613,\n",
       " 1961: 0.8999908164203134,\n",
       " 4178: 0.8996234732298944,\n",
       " 2010: 0.8995316374323465,\n",
       " 4227: 0.8991642942419276,\n",
       " 2059: 0.8990724584443797,\n",
       " 4276: 0.8987051152539607,\n",
       " 2108: 0.8986132794564128,\n",
       " 4325: 0.8982459362659938,\n",
       " 2157: 0.8981541004684459,\n",
       " 4374: 0.8977867572780269,\n",
       " 2206: 0.897694921480479,\n",
       " 38: 0.8976030856828032,\n",
       " 4423: 0.89732757829006,\n",
       " 2255: 0.8972357424922848,\n",
       " 87: 0.8971439066948363,\n",
       " 4472: 0.8968683993025479,\n",
       " 2304: 0.8967765635043179,\n",
       " 136: 0.8966847277068695,\n",
       " 4521: 0.8964092203141263,\n",
       " 2353: 0.896317384516351,\n",
       " 185: 0.8962255487188884,\n",
       " 4570: 0.8959500413266142,\n",
       " 2402: 0.8958582055283841,\n",
       " 234: 0.8957663697309215,\n",
       " 4619: 0.8954908623381925,\n",
       " 2451: 0.8953990265404173,\n",
       " 283: 0.895307190742983,\n",
       " 4668: 0.8950316833506804,\n",
       " 2500: 0.8949398475524504,\n",
       " 332: 0.8948480117550162,\n",
       " 4717: 0.8945725043622588,\n",
       " 2549: 0.8944806685644835,\n",
       " 381: 0.8943888327669924,\n",
       " 4766: 0.8941133253747466,\n",
       " 2598: 0.8940214895765166,\n",
       " 430: 0.8939296537790256,\n",
       " 4815: 0.893654146386325,\n",
       " 2647: 0.8935623105885497,\n",
       " 479: 0.8934704747910587,\n",
       " 4864: 0.8931949673988129,\n",
       " 2696: 0.8931031316005829,\n",
       " 528: 0.8930112958030918,\n",
       " 4913: 0.8927357884103913,\n",
       " 2745: 0.892643952612616,\n",
       " 577: 0.8925521168151818,\n",
       " 4962: 0.8922766094228791,\n",
       " 2794: 0.8921847736246491,\n",
       " 626: 0.8920929378272149,\n",
       " 2843: 0.8917255946366822,\n",
       " 675: 0.891633758839248,\n",
       " 2892: 0.8912664156487153,\n",
       " 724: 0.8911745798512811,\n",
       " 2941: 0.8908072366607485,\n",
       " 773: 0.8907154008633142,\n",
       " 2990: 0.8903480576727816,\n",
       " 822: 0.8902562218753474,\n",
       " 3039: 0.8898888786848147,\n",
       " 871: 0.8897970428873805,\n",
       " 3088: 0.8894296996968478,\n",
       " 920: 0.8893378638994136,\n",
       " 3137: 0.8889705207088809,\n",
       " 969: 0.8888786849114467,\n",
       " 3186: 0.8885113417209141,\n",
       " 1018: 0.8884195059234798,\n",
       " 3235: 0.8880521627329472,\n",
       " 1067: 0.887960326935513,\n",
       " 3284: 0.8875929837449803,\n",
       " 1116: 0.8875011479474324,\n",
       " 3333: 0.8871338047570134,\n",
       " 1165: 0.8870419689594655,\n",
       " 3382: 0.8866746257690465,\n",
       " 1214: 0.8865827899714986,\n",
       " 3431: 0.8862154467810797,\n",
       " 1263: 0.8861236109835318,\n",
       " 3480: 0.8857562677931128,\n",
       " 1312: 0.8856644319955649,\n",
       " 3529: 0.8852970888051459,\n",
       " 1361: 0.885205253007598,\n",
       " 3578: 0.884837909817179,\n",
       " 1410: 0.8847460740196311,\n",
       " 3627: 0.8843787308292121,\n",
       " 1459: 0.8842868950316642,\n",
       " 3676: 0.8839195518412453,\n",
       " 1508: 0.8838277160436974,\n",
       " 3725: 0.8834603728532784,\n",
       " 1557: 0.8833685370557305,\n",
       " 3774: 0.8830011938653115,\n",
       " 1606: 0.8829093580677636,\n",
       " 3823: 0.8825420148773446,\n",
       " 1655: 0.8824501790797967,\n",
       " 3872: 0.8820828358893777,\n",
       " 1704: 0.8819910000918298,\n",
       " 3921: 0.8816236569014109,\n",
       " 1753: 0.881531821103863,\n",
       " 3970: 0.881164477913444,\n",
       " 1802: 0.8810726421158961,\n",
       " 4019: 0.8807052989254771,\n",
       " 1851: 0.8806134631279292,\n",
       " 4068: 0.8802461199375102,\n",
       " 1900: 0.8801542841399623,\n",
       " 4117: 0.8797869409495434,\n",
       " 1949: 0.8796951051519954,\n",
       " 4166: 0.8793277619615765,\n",
       " 1998: 0.8792359261640286,\n",
       " 4215: 0.8788685829736096,\n",
       " 2047: 0.8787767471760617,\n",
       " 4264: 0.8784094039856427,\n",
       " 2096: 0.8783175681880948,\n",
       " 4313: 0.8779502249976758,\n",
       " 2145: 0.8778583892001279,\n",
       " 4362: 0.877491046009709,\n",
       " 2194: 0.877399210212161,\n",
       " 26: 0.8773073744145456,\n",
       " 4411: 0.8770318670217421,\n",
       " 2243: 0.8769400312244215,\n",
       " 75: 0.8768481954265752,\n",
       " 4460: 0.8765726880337752,\n",
       " 2292: 0.8764808522364547,\n",
       " 124: 0.8763890164386083,\n",
       " 4509: 0.8761135090462631,\n",
       " 2341: 0.8760216732484878,\n",
       " 173: 0.8759298374506272,\n",
       " 4558: 0.8756543300578414,\n",
       " 2390: 0.8755624942605209,\n",
       " 222: 0.8754706584626604,\n",
       " 4607: 0.8751951510703293,\n",
       " 2439: 0.875103315272554,\n",
       " 271: 0.8750114794746935,\n",
       " 4656: 0.8747359720819077,\n",
       " 2488: 0.8746441362845871,\n",
       " 320: 0.874552300486755,\n",
       " 4705: 0.8742767930943955,\n",
       " 2537: 0.8741849572966203,\n",
       " 369: 0.8740931214987882,\n",
       " 4754: 0.8738176141059739,\n",
       " 2586: 0.8737257783086534,\n",
       " 418: 0.8736339425108213,\n",
       " 4803: 0.8733584351184618,\n",
       " 2635: 0.8732665993206865,\n",
       " 467: 0.8731747635228544,\n",
       " 4852: 0.8728992561300402,\n",
       " 2684: 0.8728074203327196,\n",
       " 516: 0.8727155845348875,\n",
       " 4901: 0.872440077142528,\n",
       " 2733: 0.872348241344298,\n",
       " 565: 0.8722564055468638,\n",
       " 4950: 0.8719808981541064,\n",
       " 2782: 0.8718890623563311,\n",
       " 614: 0.8717972265588969,\n",
       " 4999: 0.8715217191665943,\n",
       " 2831: 0.8714298833683642,\n",
       " 663: 0.87133804757093,\n",
       " 2880: 0.8709707043803974,\n",
       " 712: 0.8708788685829632,\n",
       " 2929: 0.8705115253924305,\n",
       " 761: 0.8704196895949963,\n",
       " 2978: 0.8700523464044636,\n",
       " 810: 0.8699605106070294,\n",
       " 3027: 0.8695931674164967,\n",
       " 859: 0.8695013316190625,\n",
       " 3076: 0.8691339884285298,\n",
       " 908: 0.8690421526310956,\n",
       " 3125: 0.868674809440563,\n",
       " 957: 0.8685829736431288,\n",
       " 3174: 0.8682156304525961,\n",
       " 1006: 0.8681237946551619,\n",
       " 3223: 0.8677564514646292,\n",
       " 1055: 0.867664615667195,\n",
       " 3272: 0.8672972724766623,\n",
       " 1104: 0.8672054366792281,\n",
       " 3321: 0.8668380934886954,\n",
       " 1153: 0.8667462576913749,\n",
       " 3370: 0.8663789145007286,\n",
       " 1202: 0.866287078703408,\n",
       " 3419: 0.8659197355127617,\n",
       " 1251: 0.8658278997154412,\n",
       " 3468: 0.8654605565247948,\n",
       " 1300: 0.8653687207274743,\n",
       " 3517: 0.8650013775368279,\n",
       " 1349: 0.8649095417395074,\n",
       " 3566: 0.864542198548861,\n",
       " 1398: 0.8644503627513132,\n",
       " 3615: 0.8640830195608942,\n",
       " 1447: 0.8639911837633463,\n",
       " 3664: 0.8636238405729273,\n",
       " 1496: 0.8635320047753794,\n",
       " 3713: 0.8631646615849604,\n",
       " 1545: 0.8630728257874125,\n",
       " 3762: 0.8627054825969935,\n",
       " 1594: 0.8626136467994456,\n",
       " 3811: 0.8622463036090267,\n",
       " 1643: 0.8621544678114788,\n",
       " 3860: 0.8617871246210598,\n",
       " 1692: 0.8616952888235119,\n",
       " 3909: 0.8613279456330929,\n",
       " 1741: 0.861236109835545,\n",
       " 3958: 0.860868766645126,\n",
       " 1790: 0.8607769308475781,\n",
       " 4007: 0.8604095876571591,\n",
       " 1839: 0.8603177518596112,\n",
       " 4056: 0.8599504086691923,\n",
       " 1888: 0.8598585728716444,\n",
       " 4105: 0.8594912296812254,\n",
       " 1937: 0.8593993938836775,\n",
       " 4154: 0.8590320506932585,\n",
       " 1986: 0.8589402148957106,\n",
       " 4203: 0.8585728717052916,\n",
       " 2035: 0.8584810359077437,\n",
       " 4252: 0.8581136927173247,\n",
       " 2084: 0.8580218569197768,\n",
       " 4301: 0.8576545137293579,\n",
       " 2133: 0.85756267793181,\n",
       " 4350: 0.857195334741391,\n",
       " 2182: 0.8571034989438431,\n",
       " 14: 0.8570116631462952,\n",
       " 4399: 0.8567361557534241,\n",
       " 2231: 0.8566443199561036,\n",
       " 63: 0.8565524841583283,\n",
       " 4448: 0.8562769767654572,\n",
       " 2280: 0.8561851409681367,\n",
       " 112: 0.8560933051703614,\n",
       " 4497: 0.8558177977774903,\n",
       " 2329: 0.8557259619801698,\n",
       " 161: 0.8556341261823945,\n",
       " 4546: 0.8553586187899782,\n",
       " 2378: 0.8552667829922029,\n",
       " 210: 0.8551749471944277,\n",
       " 4595: 0.8548994398015566,\n",
       " 2427: 0.854807604004236,\n",
       " 259: 0.8547157682064608,\n",
       " 4644: 0.8544402608140444,\n",
       " 2476: 0.8543484250162692,\n",
       " 308: 0.8542565892184939,\n",
       " 4693: 0.8539810818256228,\n",
       " 2525: 0.8538892460283023,\n",
       " 357: 0.853797410230527,\n",
       " 4742: 0.8535219028381107,\n",
       " 2574: 0.8534300670403354,\n",
       " 406: 0.8533382312425601,\n",
       " 4791: 0.8530627238496891,\n",
       " 2623: 0.8529708880523685,\n",
       " 455: 0.8528790522545933,\n",
       " 4840: 0.8526035448621769,\n",
       " 2672: 0.8525117090644017,\n",
       " 504: 0.8524198732666264,\n",
       " 4889: 0.8521443658737553,\n",
       " 2721: 0.8520525300764348,\n",
       " 553: 0.8519606942786595,\n",
       " 4938: 0.8516851868862432,\n",
       " 2770: 0.8515933510884679,\n",
       " 602: 0.8515015152906926,\n",
       " 4987: 0.8512260078978215,\n",
       " 2819: 0.851134172100501,\n",
       " 651: 0.8510423363027257,\n",
       " 2868: 0.8506749931125341,\n",
       " 700: 0.8505831573147589,\n",
       " 2917: 0.8502158141245673,\n",
       " 749: 0.850123978326792,\n",
       " 2966: 0.8497566351366004,\n",
       " 798: 0.8496647993388251,\n",
       " 3015: 0.8492974561486335,\n",
       " 847: 0.8492056203508582,\n",
       " 3064: 0.8488382771606666,\n",
       " 896: 0.8487464413628913,\n",
       " 3113: 0.8483790981726997,\n",
       " 945: 0.8482872623749245,\n",
       " 3162: 0.8479199191847329,\n",
       " 994: 0.8478280833869576,\n",
       " 3211: 0.847460740196766,\n",
       " 1043: 0.8473689043989907,\n",
       " 3260: 0.8470015612087991,\n",
       " 1092: 0.8469097254110238,\n",
       " 3309: 0.8465423822208322,\n",
       " 1141: 0.846450546423057,\n",
       " 3358: 0.8460832032328653,\n",
       " 1190: 0.8459913674350901,\n",
       " 3407: 0.8456240242448985,\n",
       " 1239: 0.8455321884471232,\n",
       " 3456: 0.8451648452569316,\n",
       " 1288: 0.8450730094591563,\n",
       " 3505: 0.8447056662689647,\n",
       " 1337: 0.8446138304711894,\n",
       " 3554: 0.8442464872809978,\n",
       " 1386: 0.8441546514832226,\n",
       " 3603: 0.843787308293031,\n",
       " 1435: 0.8436954724952557,\n",
       " 3652: 0.8433281293050641,\n",
       " 1484: 0.8432362935072888,\n",
       " 3701: 0.8428689503170972,\n",
       " 1533: 0.8427771145193219,\n",
       " 3750: 0.8424097713291303,\n",
       " 1582: 0.842317935531355,\n",
       " 3799: 0.8419505923411634,\n",
       " 1631: 0.8418587565433882,\n",
       " 3848: 0.8414914133531965,\n",
       " 1680: 0.8413995775554213,\n",
       " 3897: 0.8410322343652297,\n",
       " 1729: 0.8409403985674544,\n",
       " 3946: 0.8405730553772628,\n",
       " 1778: 0.8404812195794875,\n",
       " 3995: 0.8401138763892959,\n",
       " 1827: 0.8400220405915206,\n",
       " 4044: 0.839654697401329,\n",
       " 1876: 0.8395628616035538,\n",
       " 4093: 0.8391955184133622,\n",
       " 1925: 0.8391036826155869,\n",
       " 4142: 0.8387363394253953,\n",
       " 1974: 0.83864450362762,\n",
       " 4191: 0.8382771604374284,\n",
       " 2023: 0.8381853246396531,\n",
       " 4240: 0.8378179814494615,\n",
       " 2072: 0.8377261456516862,\n",
       " 4289: 0.8373588024614946,\n",
       " 2121: 0.8372669666637194,\n",
       " 4338: 0.8368996234735278,\n",
       " 2170: 0.8368077876757525,\n",
       " 2: 0.836715951878042,\n",
       " 4387: 0.8364404444855609,\n",
       " 2219: 0.8363486086877856,\n",
       " 51: 0.8362567728900743,\n",
       " 4436: 0.835981265497594,\n",
       " 2268: 0.8358894296998187,\n",
       " 100: 0.8357975939021003,\n",
       " 4485: 0.8355220865096271,\n",
       " 2317: 0.8354302507118518,\n",
       " 149: 0.8353384149141334,\n",
       " 4534: 0.8350629075212055,\n",
       " 2366: 0.834971071723885,\n",
       " 198: 0.8348792359261665,\n",
       " 4583: 0.8346037285336934,\n",
       " 2415: 0.8345118927359181,\n",
       " 247: 0.8344200569381997,\n",
       " 4632: 0.8341445495452717,\n",
       " 2464: 0.8340527137479512,\n",
       " 296: 0.8339608779502328,\n",
       " 4681: 0.8336853705577596,\n",
       " 2513: 0.8335935347599843,\n",
       " 345: 0.8335016989622659,\n",
       " 4730: 0.833226191569338,\n",
       " 2562: 0.8331343557720174,\n",
       " 394: 0.833042519974299,\n",
       " 4779: 0.8327670125818258,\n",
       " 2611: 0.8326751767840506,\n",
       " 443: 0.8325833409863321,\n",
       " 4828: 0.8323078335934042,\n",
       " 2660: 0.8322159977960837,\n",
       " 492: 0.8321241619983653,\n",
       " 4877: 0.8318486546058921,\n",
       " 2709: 0.8317568188081168,\n",
       " 541: 0.8316649830103984,\n",
       " 4926: 0.8313894756174705,\n",
       " 2758: 0.8312976398201499,\n",
       " 590: 0.8312058040223747,\n",
       " 4975: 0.8309302966299583,\n",
       " 2807: 0.830838460832183,\n",
       " 639: 0.8307466250344078,\n",
       " 2856: 0.8303792818442162,\n",
       " 688: 0.8302874460464409,\n",
       " 2905: 0.8299201028562493,\n",
       " 737: 0.829828267058474,\n",
       " 2954: 0.8294609238682824,\n",
       " 786: 0.8293690880705071,\n",
       " 3003: 0.8290017448803155,\n",
       " 835: 0.8289099090825403,\n",
       " 3052: 0.8285425658923486,\n",
       " 884: 0.8284507300945734,\n",
       " 3101: 0.8280833869043818,\n",
       " 933: 0.8279915511066065,\n",
       " 3150: 0.8276242079164149,\n",
       " 982: 0.8275323721186396,\n",
       " 3199: 0.827165028928448,\n",
       " 1031: 0.8270731931306727,\n",
       " 3248: 0.8267058499404811,\n",
       " 1080: 0.8266140141427059,\n",
       " 3297: 0.8262466709525143,\n",
       " 1129: 0.826154835154739,\n",
       " 3346: 0.8257874919645474,\n",
       " 1178: 0.8256956561667721,\n",
       " 3395: 0.8253283129765805,\n",
       " 1227: 0.8252364771788052,\n",
       " 3444: 0.8248691339886136,\n",
       " 1276: 0.8247772981908383,\n",
       " 3493: 0.8244099550006467,\n",
       " 1325: 0.8243181192028715,\n",
       " 3542: 0.8239507760126799,\n",
       " 1374: 0.8238589402149046,\n",
       " 3591: 0.823491597024713,\n",
       " 1423: 0.8233997612269377,\n",
       " 3640: 0.8230324180367461,\n",
       " 1472: 0.8229405822389708,\n",
       " 3689: 0.8225732390487792,\n",
       " 1521: 0.8224814032510039,\n",
       " 3738: 0.8221140600608123,\n",
       " 1570: 0.8220222242630371,\n",
       " 3787: 0.8216548810728455,\n",
       " 1619: 0.8215630452750702,\n",
       " 3836: 0.8211957020848786,\n",
       " 1668: 0.8211038662871033,\n",
       " 3885: 0.8207365230969117,\n",
       " 1717: 0.8206446872991364,\n",
       " 3934: 0.8202773441089448,\n",
       " 1766: 0.8201855083111695,\n",
       " 3983: 0.8198181651209779,\n",
       " 1815: 0.8197263293232027,\n",
       " 4032: 0.8193589861330111,\n",
       " 1864: 0.8192671503352358,\n",
       " 4081: 0.8188998071450442,\n",
       " 1913: 0.8188079713472689,\n",
       " 4130: 0.8184406281570773,\n",
       " 1962: 0.818348792359302,\n",
       " 4179: 0.8179814491691104,\n",
       " 2011: 0.8178896133713351,\n",
       " 4228: 0.8175222701811435,\n",
       " 2060: 0.8174304343833683,\n",
       " 4277: 0.8170630911931767,\n",
       " 2109: 0.8169712553954014,\n",
       " 4326: 0.8166039122052098,\n",
       " 2158: 0.8165120764074345,\n",
       " 4375: 0.8161447332172429,\n",
       " 2207: 0.8160528974194676,\n",
       " 39: 0.8159610616218202,\n",
       " 4424: 0.815685554229276,\n",
       " 2256: 0.8155937184315007,\n",
       " 88: 0.8155018826338534,\n",
       " 4473: 0.8152263752408544,\n",
       " 2305: 0.8151345394435339,\n",
       " 137: 0.8150427036458865,\n",
       " 4522: 0.8147671962533423,\n",
       " 2354: 0.814675360455567,\n",
       " 186: 0.8145835246579054,\n",
       " 4571: 0.8143080172649206,\n",
       " 2403: 0.8142161814676001,\n",
       " 235: 0.8141243456699385,\n",
       " 4620: 0.8138488382774085,\n",
       " 2452: 0.8137570024796332,\n",
       " 284: 0.8136651666819716,\n",
       " 4669: 0.8133896592889869,\n",
       " 2501: 0.8132978234916663,\n",
       " 333: 0.8132059876940048,\n",
       " 4718: 0.8129304803014747,\n",
       " 2550: 0.8128386445036995,\n",
       " 382: 0.8127468087060379,\n",
       " 4767: 0.8124713013130531,\n",
       " 2599: 0.8123794655157326,\n",
       " 431: 0.812287629718071,\n",
       " 4816: 0.812012122325541,\n",
       " 2648: 0.8119202865277657,\n",
       " 480: 0.8118284507301041,\n",
       " 4865: 0.8115529433371194,\n",
       " 2697: 0.8114611075397988,\n",
       " 529: 0.8113692717421372,\n",
       " 4914: 0.8110937643496072,\n",
       " 2746: 0.811001928551832,\n",
       " 578: 0.8109100927541704,\n",
       " 4963: 0.8106345853611856,\n",
       " 2795: 0.8105427495638651,\n",
       " 627: 0.8104509137662035,\n",
       " 2844: 0.8100835705758982,\n",
       " 676: 0.8099917347782366,\n",
       " 2893: 0.8096243915879313,\n",
       " 725: 0.8095325557902697,\n",
       " 2942: 0.8091652125999644,\n",
       " 774: 0.8090733768023028,\n",
       " 2991: 0.8087060336119976,\n",
       " 823: 0.808614197814336,\n",
       " 3040: 0.8082468546240307,\n",
       " 872: 0.8081550188263691,\n",
       " 3089: 0.8077876756360638,\n",
       " 921: 0.8076958398384022,\n",
       " 3138: 0.8073284966480969,\n",
       " 970: 0.8072366608504353,\n",
       " 3187: 0.80686931766013,\n",
       " 1019: 0.8067774818624684,\n",
       " 3236: 0.8064101386721632,\n",
       " 1068: 0.8063183028745016,\n",
       " 3285: 0.8059509596841963,\n",
       " 1117: 0.805859123886421,\n",
       " 3334: 0.8054917806962294,\n",
       " 1166: 0.8053999448984541,\n",
       " 3383: 0.8050326017082625,\n",
       " 1215: 0.8049407659104872,\n",
       " 3432: 0.8045734227202956,\n",
       " 1264: 0.8044815869225204,\n",
       " 3481: 0.8041142437323288,\n",
       " 1313: 0.8040224079345535,\n",
       " 3530: 0.8036550647443619,\n",
       " 1362: 0.8035632289465866,\n",
       " 3579: 0.803195885756395,\n",
       " 1411: 0.8031040499586197,\n",
       " 3628: 0.8027367067684281,\n",
       " 1460: 0.8026448709706528,\n",
       " 3677: 0.8022775277804612,\n",
       " 1509: 0.802185691982686,\n",
       " 3726: 0.8018183487924944,\n",
       " 1558: 0.8017265129947191,\n",
       " 3775: 0.8013591698045275,\n",
       " 1607: 0.8012673340067522,\n",
       " 3824: 0.8008999908165606,\n",
       " 1656: 0.8008081550187853,\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# order dict from highest to lowest\n",
    "def dict_sort(d):\n",
    "    return dict(sorted(d.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# print dict in order\n",
    "dict_sort(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 8s 1us/step\n",
      "Fit model on training data\n",
      "Epoch 1/2\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.3392 - sparse_categorical_accuracy: 0.9039 - val_loss: 0.1867 - val_sparse_categorical_accuracy: 0.9466\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.1591 - sparse_categorical_accuracy: 0.9523 - val_loss: 0.1314 - val_sparse_categorical_accuracy: 0.9630\n",
      "Evaluate model on test data\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.1302 - sparse_categorical_accuracy: 0.9603\n",
      "test loss, test acc: [0.13022162020206451, 0.9603000283241272]\n",
      "Generate a prediction\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "prediction shape: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries required in this example:\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess the data (NumPy arrays):\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255\n",
    "\n",
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")\n",
    "\n",
    "# Allocate 10,000 samples for validation:\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(),  # Optimizer\n",
    "    # Minimize loss:\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    # Monitor metrics:\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "print(\"Fit model on training data\")\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs=2,\n",
    "    # Validation of loss and metrics\n",
    "    # at the end of each epoch:\n",
    "    validation_data=(x_val, y_val),\n",
    ")\n",
    "\n",
    "history.history\n",
    "\n",
    "print(\"Evaluate model on test data\")\n",
    "results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "# Generate a prediction using model.predict() \n",
    "# and calculate it's shape:\n",
    "print(\"Generate a prediction\")\n",
    "prediction = model.predict(x_test[:1])\n",
    "print(\"prediction shape:\", prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.5909412e-06, 4.3930575e-07, 1.8799146e-04, 2.0909435e-04,\n",
       "        6.2949766e-08, 7.2605559e-07, 6.7664069e-11, 9.9947935e-01,\n",
       "        2.2602323e-06, 1.1346632e-04]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "legal_actions = [0,3,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(prediction.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 6.590941e-06, 3: 0.00020909435, 5: 7.260556e-07}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{action: prediction.flatten()[action] for action in legal_actions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {action: prediction.flatten()[action] for action in legal_actions}\n",
    "\n",
    "# take the key of the dictionary that contains the highest number\n",
    "# if the values of the dictionary tied, take a random key\n",
    "best_action = max(dictionary, key=dictionary.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_actions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[:,:,:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6989700043360187"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "math.log(5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base: 2, precision_big: 0.05, precision_low: 0.12, importance: 2.4\n",
      "base: 3, precision_big: 0.07, precision_low: 0.14, importance: 2.0\n",
      "base: 4, precision_big: 0.08, precision_low: 0.14, importance: 1.75\n",
      "base: 5, precision_big: 0.08, precision_low: 0.13, importance: 1.62\n",
      "base: 6, precision_big: 0.08, precision_low: 0.12, importance: 1.5\n",
      "base: 7, precision_big: 0.08, precision_low: 0.11, importance: 1.38\n",
      "base: 8, precision_big: 0.07, precision_low: 0.1, importance: 1.43\n",
      "base: 9, precision_big: 0.07, precision_low: 0.09, importance: 1.29\n",
      "base: 10, precision_big: 0.07, precision_low: 0.08, importance: 1.14\n",
      "base: 11, precision_big: 0.06, precision_low: 0.08, importance: 1.33\n",
      "base: 12, precision_big: 0.06, precision_low: 0.07, importance: 1.17\n",
      "base: 13, precision_big: 0.06, precision_low: 0.07, importance: 1.17\n",
      "base: 14, precision_big: 0.05, precision_low: 0.06, importance: 1.2\n",
      "base: 15, precision_big: 0.05, precision_low: 0.06, importance: 1.2\n",
      "base: 16, precision_big: 0.05, precision_low: 0.06, importance: 1.2\n",
      "base: 17, precision_big: 0.05, precision_low: 0.05, importance: 1.0\n",
      "base: 18, precision_big: 0.05, precision_low: 0.05, importance: 1.0\n",
      "base: 19, precision_big: 0.04, precision_low: 0.05, importance: 1.25\n",
      "base: 20, precision_big: 0.04, precision_low: 0.05, importance: 1.25\n",
      "base: 21, precision_big: 0.04, precision_low: 0.04, importance: 1.0\n",
      "base: 22, precision_big: 0.04, precision_low: 0.04, importance: 1.0\n",
      "base: 23, precision_big: 0.04, precision_low: 0.04, importance: 1.0\n",
      "base: 24, precision_big: 0.04, precision_low: 0.04, importance: 1.0\n",
      "base: 25, precision_big: 0.03, precision_low: 0.04, importance: 1.33\n",
      "base: 26, precision_big: 0.03, precision_low: 0.04, importance: 1.33\n",
      "base: 27, precision_big: 0.03, precision_low: 0.04, importance: 1.33\n",
      "base: 28, precision_big: 0.03, precision_low: 0.03, importance: 1.0\n",
      "base: 29, precision_big: 0.03, precision_low: 0.03, importance: 1.0\n",
      "base: 30, precision_big: 0.03, precision_low: 0.03, importance: 1.0\n",
      "base: 31, precision_big: 0.03, precision_low: 0.03, importance: 1.0\n",
      "base: 32, precision_big: 0.03, precision_low: 0.03, importance: 1.0\n",
      "base: 33, precision_big: 0.03, precision_low: 0.03, importance: 1.0\n",
      "base: 34, precision_big: 0.03, precision_low: 0.03, importance: 1.0\n",
      "base: 35, precision_big: 0.03, precision_low: 0.03, importance: 1.0\n",
      "base: 36, precision_big: 0.03, precision_low: 0.03, importance: 1.0\n",
      "base: 37, precision_big: 0.02, precision_low: 0.03, importance: 1.5\n",
      "base: 38, precision_big: 0.02, precision_low: 0.03, importance: 1.5\n",
      "base: 39, precision_big: 0.02, precision_low: 0.02, importance: 1.0\n",
      "base: 40, precision_big: 0.02, precision_low: 0.02, importance: 1.0\n",
      "base: 41, precision_big: 0.02, precision_low: 0.02, importance: 1.0\n",
      "base: 42, precision_big: 0.02, precision_low: 0.02, importance: 1.0\n",
      "base: 43, precision_big: 0.02, precision_low: 0.02, importance: 1.0\n",
      "base: 44, precision_big: 0.02, precision_low: 0.02, importance: 1.0\n",
      "base: 45, precision_big: 0.02, precision_low: 0.02, importance: 1.0\n",
      "base: 46, precision_big: 0.02, precision_low: 0.02, importance: 1.0\n",
      "base: 47, precision_big: 0.02, precision_low: 0.02, importance: 1.0\n",
      "base: 48, precision_big: 0.02, precision_low: 0.02, importance: 1.0\n",
      "base: 49, precision_big: 0.02, precision_low: 0.02, importance: 1.0\n"
     ]
    }
   ],
   "source": [
    "big_number = 100000\n",
    "low_number = 100\n",
    "\n",
    "for base in range(2,50):\n",
    "    precision_big = round(1/math.log(big_number, base), 2)\n",
    "    precision_low = round(1/math.log(low_number, base), 2)\n",
    "    importance = round(precision_low/precision_big,2)\n",
    "\n",
    "    print(f'base: {base}, precision_big: {precision_big}, precision_low: {precision_low}, importance: {importance}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "math range error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m low_number \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m base \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m,\u001b[39m50\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     precision_big \u001b[39m=\u001b[39m \u001b[39mround\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m/\u001b[39mmath\u001b[39m.\u001b[39;49mexp(big_number), \u001b[39m2\u001b[39m)\n\u001b[1;32m      6\u001b[0m     precision_low \u001b[39m=\u001b[39m \u001b[39mround\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m/\u001b[39mmath\u001b[39m.\u001b[39mexp(low_number), \u001b[39m2\u001b[39m)\n\u001b[1;32m      7\u001b[0m     importance \u001b[39m=\u001b[39m \u001b[39mround\u001b[39m(precision_low\u001b[39m/\u001b[39mprecision_big,\u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mOverflowError\u001b[0m: math range error"
     ]
    }
   ],
   "source": [
    "big_number = 100000\n",
    "low_number = 100\n",
    "\n",
    "for base in range(2,50):\n",
    "    precision_big = round(1/math.exp(big_number), 2)\n",
    "    precision_low = round(1/math.exp(low_number), 2)\n",
    "    importance = round(precision_low/precision_big,2)\n",
    "\n",
    "    print(f'base: {base}, precision_big: {precision_big}, precision_low: {precision_low}, importance: {importance}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1622776601683795"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100000**(1/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09993741652368077"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/math.log(100000, 3.16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2498435413092019"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/math.log(100,3.16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.25/0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/math.log(low_number, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "minitem = 30\n",
    "constraint_value = math.log(minitem)\n",
    "bigger_value = math.log(100000)\n",
    "lower_value = math.log(100)\n",
    "\n",
    "formula = 1#-constraint_value + 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6537308853024272\n"
     ]
    }
   ],
   "source": [
    "big_value = 300000\n",
    "small_value = 30\n",
    "formula = 30\n",
    "\n",
    "def calculate_weight(value):\n",
    "    # bias = (math.log(1000*30) - 100*math.log(30))/9\n",
    "    return (1/(-formula+0.00001+math.log(value)))\n",
    "\n",
    "print(calculate_weight(small_value)/calculate_weight(big_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower 0.0 weight low 1.0 weight high 2.5\n",
      "lower 1.0 weight low 1.0 weight high 2.92\n",
      "lower 2.0 weight low 1.0 weight high 3.65\n",
      "lower 3.0 weight low 1.0 weight high 5.3\n",
      "lower 4.0 weight low 1.0 weight high 12.41\n",
      "lower 5.0 weight low 1.0 weight high -16.5\n",
      "lower 6.0 weight low 1.0 weight high -3.95\n",
      "lower 7.0 weight low 1.0 weight high -1.88\n",
      "lower 8.0 weight low 1.0 weight high -1.03\n",
      "lower 9.0 weight low 1.0 weight high -0.57\n",
      "lower 10.0 weight low 1.0 weight high -0.28\n",
      "lower 11.0 weight low 1.0 weight high -0.08\n",
      "lower 12.0 weight low 1.0 weight high 0.07\n",
      "lower 13.0 weight low 1.0 weight high 0.18\n",
      "lower 14.0 weight low 1.0 weight high 0.26\n",
      "lower 15.0 weight low 1.0 weight high 0.34\n",
      "lower 16.0 weight low 1.0 weight high 0.39\n",
      "lower 17.0 weight low 1.0 weight high 0.44\n",
      "lower 18.0 weight low 1.0 weight high 0.48\n",
      "lower 19.0 weight low 1.0 weight high 0.52\n",
      "lower 20.0 weight low 1.0 weight high 0.55\n",
      "lower 21.0 weight low 1.0 weight high 0.58\n",
      "lower 22.0 weight low 1.0 weight high 0.6\n",
      "lower 23.0 weight low 1.0 weight high 0.62\n",
      "lower 24.0 weight low 1.0 weight high 0.64\n",
      "lower 25.0 weight low 1.0 weight high 0.66\n",
      "lower 26.0 weight low 1.0 weight high 0.68\n",
      "lower 27.0 weight low 1.0 weight high 0.69\n",
      "lower 28.0 weight low 1.0 weight high 0.7\n",
      "lower 29.0 weight low 1.0 weight high 0.72\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "results = []\n",
    "\n",
    "for formula in np.arange(0.0,30.0, 1):\n",
    "    def calculate_weight(value, formula):\n",
    "        # bias = (math.log(1000*30) - 100*math.log(30))/9\n",
    "        return (1/(-formula+0.0001+math.log(value)))\n",
    "    \n",
    "    big_value = 30\n",
    "    small_value = 30\n",
    "    big_value2 = 100000\n",
    "    small_value2 = 100\n",
    "    print(\n",
    "        'lower', \n",
    "        formula, \n",
    "        'weight low', \n",
    "        round(calculate_weight(small_value, formula)/calculate_weight(big_value, formula),2),\n",
    "        'weight high', \n",
    "        round(calculate_weight(small_value2, formula)/calculate_weight(big_value2, formula),2)\n",
    "    )\n",
    "    dict_results = {\n",
    "            'bias': formula, \n",
    "            'weight low': round(calculate_weight(small_value, formula)/calculate_weight(big_value, formula),2),\n",
    "            'weight high': round(calculate_weight(small_value2, formula)/calculate_weight(big_value2, formula),2)\n",
    "        }\n",
    "    results.append(dict_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>weight low</th>\n",
       "      <th>weight high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-16.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    bias  weight low  weight high\n",
       "0    0.0         1.0         2.50\n",
       "1    1.0         1.0         2.92\n",
       "2    2.0         1.0         3.65\n",
       "3    3.0         1.0         5.30\n",
       "4    4.0         1.0        12.41\n",
       "5    5.0         1.0       -16.50\n",
       "6    6.0         1.0        -3.95\n",
       "7    7.0         1.0        -1.88\n",
       "8    8.0         1.0        -1.03\n",
       "9    9.0         1.0        -0.57\n",
       "10  10.0         1.0        -0.28\n",
       "11  11.0         1.0        -0.08\n",
       "12  12.0         1.0         0.07\n",
       "13  13.0         1.0         0.18\n",
       "14  14.0         1.0         0.26\n",
       "15  15.0         1.0         0.34\n",
       "16  16.0         1.0         0.39\n",
       "17  17.0         1.0         0.44\n",
       "18  18.0         1.0         0.48\n",
       "19  19.0         1.0         0.52\n",
       "20  20.0         1.0         0.55\n",
       "21  21.0         1.0         0.58\n",
       "22  22.0         1.0         0.60\n",
       "23  23.0         1.0         0.62\n",
       "24  24.0         1.0         0.64\n",
       "25  25.0         1.0         0.66\n",
       "26  26.0         1.0         0.68\n",
       "27  27.0         1.0         0.69\n",
       "28  28.0         1.0         0.70\n",
       "29  29.0         1.0         0.72"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9771756663957138"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = (\n",
    "    (formula + bigger_value)/(formula + constraint_value)\n",
    ")\n",
    "\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "770.7832083416191"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_weights(value, bias):\n",
    "    return (1/(-bias+0.0001+math.log(value)))\n",
    "\n",
    "calculate_weights(30, bias=3.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weights(value, bias):\n",
    "    return (1/(-bias+math.log(value)))\n",
    "\n",
    "for i in np.arange(0.0, round(math.log(30)*0.9, 1), 0.01):\n",
    "    small_value = 30\n",
    "    big_value = 31\n",
    "\n",
    "    small_value2 = 100\n",
    "    big_value2 = 100000\n",
    "\n",
    "    if i > math.log(30):\n",
    "        print(i)\n",
    "\n",
    "    dict_results = {\n",
    "            'bias': i, \n",
    "            'weight low': round(calculate_weights(small_value, i)/calculate_weights(big_value, i),2),\n",
    "            'weight high': round(calculate_weights(small_value2, i)/calculate_weights(big_value2, i),2)\n",
    "        }\n",
    "    results.append(dict_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jaime.kuei/Documents/study-repositories/master/second-semester/reinforcement-learning/rl-chess-nova-ims'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>weight low</th>\n",
       "      <th>weight high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "      <td>1528.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.907428</td>\n",
       "      <td>1.073855</td>\n",
       "      <td>3.589993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.357576</td>\n",
       "      <td>0.957039</td>\n",
       "      <td>1.241827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-16.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.837500</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>2.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.690000</td>\n",
       "      <td>1.020000</td>\n",
       "      <td>3.315000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.530000</td>\n",
       "      <td>1.040000</td>\n",
       "      <td>4.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>28.380000</td>\n",
       "      <td>12.410000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              bias   weight low  weight high\n",
       "count  1528.000000  1528.000000  1528.000000\n",
       "mean      1.907428     1.073855     3.589993\n",
       "std       2.357576     0.957039     1.241827\n",
       "min       0.000000     1.000000   -16.500000\n",
       "25%       0.837500     1.010000     2.810000\n",
       "50%       1.690000     1.020000     3.315000\n",
       "75%       2.530000     1.040000     4.250000\n",
       "max      29.000000    28.380000    12.410000"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "#save csv\n",
    "results_df.to_csv(os.path.join(os.getcwd(), 'results.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbba8976c70>]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import library plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "#plot two lines with x axis with the column bias and y axis the columns weight low and weight high\n",
    "plt.plot(results_df['bias'], results_df['weight low'], label='bias')\n",
    "plt.plot(results_df['bias'], results_df['weight high'], label='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ifoodcorp.com.br\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in ./.reinforcement-learning-env/lib/python3.9/site-packages (from seaborn) (1.23.5)\n",
      "Requirement already satisfied: pandas>=0.25 in ./.reinforcement-learning-env/lib/python3.9/site-packages (from seaborn) (2.0.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in ./.reinforcement-learning-env/lib/python3.9/site-packages (from seaborn) (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.reinforcement-learning-env/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.reinforcement-learning-env/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.reinforcement-learning-env/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./.reinforcement-learning-env/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.reinforcement-learning-env/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./.reinforcement-learning-env/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.reinforcement-learning-env/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.reinforcement-learning-env/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./.reinforcement-learning-env/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (5.12.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.reinforcement-learning-env/lib/python3.9/site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.reinforcement-learning-env/lib/python3.9/site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.reinforcement-learning-env/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.1->seaborn) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.reinforcement-learning-env/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.12.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='bias', ylabel='weight low'>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot scatter plot with x axis with the column bias and y axis the column weight low\n",
    "import seaborn as sns\n",
    "sns.scatterplot(data=results_df, x=\"bias\", y=\"weight low\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'bar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1c/4trmvmbs6h38d0yjvlcf_cqmv3kl5x/T/ipykernel_14293/1288316524.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weight low'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/study-repositories/master/second-semester/reinforcement-learning/rl-chess-nova-ims/.reinforcement-learning-env/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5985\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5986\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5987\u001b[0m         ):\n\u001b[1;32m   5988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'bar'"
     ]
    }
   ],
   "source": [
    "results_df.bar.plot('bias', 'weight low')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".reinforcement-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
